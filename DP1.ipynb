{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWYlf89SGZBy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39NqjSnGZB3",
        "outputId": "a84b4251-bb7e-4cab-e5b7-0f07dfc4b625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 204803432.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 22415931.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 65163111.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 19864993.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Trainset size: 60000\n",
            "Testset size: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "print('Trainset size:', len(trainset))\n",
        "print('Testset size:', len(testset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXMUwQxBd19k",
        "outputId": "a156e02f-8ec8-4cb2-8ae3-12d2fc51ea89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "model = Net()\n",
        "print(model)       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9H4Jc-SaDefA"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"gpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q7GddvNMGZCC"
      },
      "outputs": [],
      "source": [
        "def get_teacher_preds(num_teachers, num_examples, epochs): \n",
        "    dirName = \"teachers \" + str(num_teachers)\n",
        "    if not os.path.exists(dirName):\n",
        "        os.mkdir(dirName)\n",
        "        \n",
        "    os.chdir(dirName) \n",
        "    \n",
        "    for i in range(num_teachers):\n",
        "        train_idx = list(range(i * num_examples, (i+1) * num_examples))\n",
        "        train = Subset(trainset, train_idx)\n",
        "        trainloader = torch.utils.data.DataLoader(train, batch_size=64, num_workers=0)      \n",
        "       \n",
        "        model = Net()\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)        \n",
        "        model.to(device)\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            running_loss = 0\n",
        "            for images, labels in trainloader:                 \n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()        \n",
        "                output = model(images)\n",
        "                loss = criterion(output, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()        \n",
        "                running_loss += loss.item() \n",
        "                \n",
        "        print(\"Teacher_\" + str(i+1) + \" training loss:\", running_loss/len(trainloader))  \n",
        "        \n",
        "        student_idx = list(range(0, 6000))      \n",
        "        student_data = Subset(testset, student_idx)  \n",
        "        student_loader = torch.utils.data.DataLoader(student_data, batch_size=64, num_workers=0)\n",
        "        \n",
        "        model.to(device) \n",
        "        model.eval()\n",
        "        \n",
        "        outputs = torch.zeros(0, dtype=torch.long).to(device)\n",
        "        for images, labels in student_loader:             \n",
        "            images, labels = images.to(device), labels.to(device)                             \n",
        "            output = model.forward(images)\n",
        "            pred = torch.argmax(torch.exp(output), dim=1)\n",
        "            outputs = torch.cat((outputs, pred))            \n",
        "    \n",
        "        file_name = 'pred_' + str(i+1) + '.pt'  \n",
        "        torch.save(outputs, file_name)          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyfFIPO1DefC",
        "outputId": "ec67ac78-9513-41b1-81e1-6daecd165102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher_1 training loss: 0.6066494191947737\n",
            "Teacher_2 training loss: 0.4426609716917339\n",
            "Teacher_3 training loss: 0.48426726225175354\n",
            "Teacher_4 training loss: 0.5373325912575972\n",
            "Teacher_5 training loss: 0.5570846247045618\n",
            "Teacher_6 training loss: 0.5038165654006758\n",
            "Teacher_7 training loss: 0.5708885632063213\n",
            "Teacher_8 training loss: 0.6416218594500893\n",
            "Teacher_9 training loss: 0.502852516738992\n",
            "Teacher_10 training loss: 0.6160007903450414\n",
            "Teacher_11 training loss: 0.6097667656446758\n",
            "Teacher_12 training loss: 0.5276183674209997\n",
            "Teacher_13 training loss: 0.6050428748130798\n",
            "Teacher_14 training loss: 0.6485544835266314\n",
            "Teacher_15 training loss: 0.6090414633876399\n",
            "Teacher_16 training loss: 0.48740353552918686\n",
            "Teacher_17 training loss: 0.6125784296738473\n",
            "Teacher_18 training loss: 0.5162034426864824\n",
            "Teacher_19 training loss: 0.5064431350482138\n",
            "Teacher_20 training loss: 0.5221488648339322\n",
            "Teacher_21 training loss: 0.5416511093315325\n",
            "Teacher_22 training loss: 0.48116329939741836\n",
            "Teacher_23 training loss: 0.5892929246551112\n",
            "Teacher_24 training loss: 0.4544162922783902\n",
            "Teacher_25 training loss: 0.520928470711959\n",
            "Teacher_26 training loss: 0.5755567754569807\n",
            "Teacher_27 training loss: 0.6335244978729048\n",
            "Teacher_28 training loss: 0.5369103237202293\n",
            "Teacher_29 training loss: 0.5330325302324797\n",
            "Teacher_30 training loss: 0.471041771926378\n",
            "Teacher_31 training loss: 0.5279193787198317\n",
            "Teacher_32 training loss: 0.611950010061264\n",
            "Teacher_33 training loss: 0.530277071814788\n",
            "Teacher_34 training loss: 0.5336536137681258\n",
            "Teacher_35 training loss: 0.5609786808490753\n",
            "Teacher_36 training loss: 0.6301469300922594\n",
            "Teacher_37 training loss: 0.5253044727601504\n",
            "Teacher_38 training loss: 0.5527997675694918\n",
            "Teacher_39 training loss: 0.5621183483224166\n",
            "Teacher_40 training loss: 0.6205755456497795\n",
            "Teacher_41 training loss: 0.46885870475518077\n",
            "Teacher_42 training loss: 0.6138697988108585\n",
            "Teacher_43 training loss: 0.5141543542083941\n",
            "Teacher_44 training loss: 0.6077237003727963\n",
            "Teacher_45 training loss: 0.5515242705219671\n",
            "Teacher_46 training loss: 0.5401558248620284\n",
            "Teacher_47 training loss: 0.49162673244350835\n",
            "Teacher_48 training loss: 0.4495469065088975\n",
            "Teacher_49 training loss: 0.4189492464065552\n",
            "Teacher_50 training loss: 0.3831997098107087\n"
          ]
        }
      ],
      "source": [
        "num_teachers = 50\n",
        "num_examples = len(trainset)//num_teachers\n",
        "epochs = 10\n",
        "\n",
        "get_teacher_preds(num_teachers, num_examples, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNstgEpPGZCK",
        "outputId": "57e41f24-9a61-4f70-ecec-8e28306dbd51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 6000)\n"
          ]
        }
      ],
      "source": [
        "num_teachers = 50\n",
        "\n",
        "preds = []\n",
        "for i in range(num_teachers): \n",
        "    file_name = 'pred_' + str(i+1) + '.pt'\n",
        "    pred = torch.load(file_name).cpu().numpy()\n",
        "    preds.append(pred)    \n",
        "    teacher_preds = np.vstack((preds))\n",
        "print(teacher_preds.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_qMGJ9IXGZCq"
      },
      "outputs": [],
      "source": [
        "def aggragate_teacher_preds(epsilon):  \n",
        "    \n",
        "    labels = np.array([]).astype(int)\n",
        "    for pred in np.transpose(teacher_preds):   \n",
        "        label_counts = np.bincount(pred, minlength=10)    \n",
        "        beta = 1 / epsilon\n",
        "\n",
        "        for i in range(len(label_counts)):\n",
        "            label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "        \n",
        "        new_label = np.argmax(label_counts)   \n",
        "        labels = np.append(labels, new_label) \n",
        "        labels =  torch.from_numpy(labels)      \n",
        "   \n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w_rhuXEzGZC4"
      },
      "outputs": [],
      "source": [
        "num_teachers, num_examples, num_labels = (50, 6000, 10)\n",
        "diff_priv_labels = aggragate_teacher_preds(0.2)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86NISKafGZDA",
        "outputId": "11889ff5-c1e4-43d5-ecf0-3abe336af45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15    Training Loss: 1.59977   \n",
            "Epoch: 2/15    Training Loss: 0.63966   \n",
            "Epoch: 3/15    Training Loss: 0.48249   \n",
            "Epoch: 4/15    Training Loss: 0.42629   \n",
            "Epoch: 5/15    Training Loss: 0.37491   \n",
            "Epoch: 6/15    Training Loss: 0.35346   \n",
            "Epoch: 7/15    Training Loss: 0.34020   \n",
            "Epoch: 8/15    Training Loss: 0.33087   \n",
            "Epoch: 9/15    Training Loss: 0.32006   \n",
            "Epoch: 10/15    Training Loss: 0.29362   \n",
            "Epoch: 11/15    Training Loss: 0.30777   \n",
            "Epoch: 12/15    Training Loss: 0.30074   \n",
            "Epoch: 13/15    Training Loss: 0.28608   \n",
            "Epoch: 14/15    Training Loss: 0.27724   \n",
            "Epoch: 15/15    Training Loss: 0.27277   \n"
          ]
        }
      ],
      "source": [
        "testset.data[:6000] = torch.FloatTensor(testset.data.clone().detach().numpy()[:6000])\n",
        "testset.targets[:6000] = diff_priv_labels\n",
        "\n",
        "student_train = Subset(testset, list(range(6000)))\n",
        "student_trainloader = torch.utils.data.DataLoader(student_train, batch_size=64, num_workers=0)\n",
        "\n",
        "model_2 = Net()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
        "model_2.to(device)\n",
        "epochs = 15\n",
        "\n",
        "for e in range(epochs): \n",
        "    train_loss = 0.0    \n",
        "    for images, labels in student_trainloader:          \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()        \n",
        "        output = model_2(images)        \n",
        "        loss = criterion(output, labels)        \n",
        "        loss.backward()       \n",
        "        optimizer.step()        \n",
        "        train_loss += loss.item()\n",
        "            \n",
        "    print(\"Epoch: {}/{}   \".format(e+1, epochs),\n",
        "                      \"Training Loss: {:.5f}   \".format(train_loss/len(student_trainloader)))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PmNoVQEGZDN",
        "outputId": "6f18dab0-8362-4081-a053-d09f6c459ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.15991    Test Accuracy: 0.95610\n"
          ]
        }
      ],
      "source": [
        "student_test = Subset(testset, list(range(6000, 10000)))  \n",
        "student_testloader = torch.utils.data.DataLoader(student_test, batch_size=64, num_workers=0)\n",
        "\n",
        "test_loss = 0\n",
        "accuracy = 0\n",
        "model_2.to(device)\n",
        "model_2.eval()\n",
        "\n",
        "with torch.no_grad(): \n",
        "    for images, labels in student_testloader:              \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model_2(images)\n",
        "        test_loss += criterion(output, labels).item()        \n",
        "               \n",
        "        ps = torch.exp(output)\n",
        "        top_p, top_class = ps.topk(1, dim=1)        \n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "print(\"Test Loss: {:.5f}   \".format(test_loss/len(student_testloader)),\n",
        "      \"Test Accuracy: {:.5f}\".format(accuracy/len(student_testloader)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L1sOFXvjO3C3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}