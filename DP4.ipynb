{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWYlf89SGZBy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39NqjSnGZB3",
        "outputId": "d409984b-863b-4636-9c6f-aff3cbee1841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 75332374.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 84005335.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27013512.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14988614.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Trainset size: 60000\n",
            "Testset size: 10000\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "print('Trainset size:', len(trainset))\n",
        "print('Testset size:', len(testset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXMUwQxBd19k",
        "outputId": "671409fc-a720-45d9-ccb4-a068fd432592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "model = Net()\n",
        "print(model)       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9H4Jc-SaDefA"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q7GddvNMGZCC"
      },
      "outputs": [],
      "source": [
        "def get_teacher_preds(num_teachers, num_examples, epochs): \n",
        "    dirName = \"teachers \" + str(num_teachers)\n",
        "    if not os.path.exists(dirName):\n",
        "        os.mkdir(dirName)\n",
        "        \n",
        "    os.chdir(dirName) \n",
        "    \n",
        "    for i in range(num_teachers):\n",
        "        train_idx = list(range(i * num_examples, (i+1) * num_examples))\n",
        "        train = Subset(trainset, train_idx)\n",
        "        trainloader = torch.utils.data.DataLoader(train, batch_size=64, num_workers=0)      \n",
        "       \n",
        "        model = Net()\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)        \n",
        "        model.to(device)\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            running_loss = 0\n",
        "            for images, labels in trainloader:                 \n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()        \n",
        "                output = model(images)\n",
        "                loss = criterion(output, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()        \n",
        "                running_loss += loss.item() \n",
        "                \n",
        "        print(\"Teacher_\" + str(i+1) + \" training loss:\", running_loss/len(trainloader))  \n",
        "        \n",
        "        student_idx = list(range(0, 6000))      \n",
        "        student_data = Subset(testset, student_idx)  \n",
        "        student_loader = torch.utils.data.DataLoader(student_data, batch_size=64, num_workers=0)\n",
        "        \n",
        "        model.to(device) \n",
        "        model.eval()\n",
        "        \n",
        "        outputs = torch.zeros(0, dtype=torch.long).to(device)\n",
        "        for images, labels in student_loader:             \n",
        "            images, labels = images.to(device), labels.to(device)                             \n",
        "            output = model.forward(images)\n",
        "            pred = torch.argmax(torch.exp(output), dim=1)\n",
        "            outputs = torch.cat((outputs, pred))            \n",
        "    \n",
        "        file_name = 'pred_' + str(i+1) + '.pt'  \n",
        "        torch.save(outputs, file_name)          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyfFIPO1DefC",
        "outputId": "a04b72e6-2cf9-44d5-dde0-5d55a18d78ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher_1 training loss: 0.595517569466641\n",
            "Teacher_2 training loss: 0.5201367657435568\n",
            "Teacher_3 training loss: 0.5701670850578108\n",
            "Teacher_4 training loss: 0.5765281507843419\n",
            "Teacher_5 training loss: 0.5290161559456273\n",
            "Teacher_6 training loss: 0.51951055620846\n",
            "Teacher_7 training loss: 0.6482827349712974\n",
            "Teacher_8 training loss: 0.6528931997324291\n",
            "Teacher_9 training loss: 0.5013592494161505\n",
            "Teacher_10 training loss: 0.5752733036091453\n",
            "Teacher_11 training loss: 0.6194542897375006\n",
            "Teacher_12 training loss: 0.5597029563627745\n",
            "Teacher_13 training loss: 0.5345853818090338\n",
            "Teacher_14 training loss: 0.5241900164830057\n",
            "Teacher_15 training loss: 0.6778721479993117\n",
            "Teacher_16 training loss: 0.5918502242941606\n",
            "Teacher_17 training loss: 0.5224397715769316\n",
            "Teacher_18 training loss: 0.5139379932692176\n",
            "Teacher_19 training loss: 0.6079388590235459\n",
            "Teacher_20 training loss: 0.5078334871091341\n",
            "Teacher_21 training loss: 0.5519323176459262\n",
            "Teacher_22 training loss: 0.44835329055786133\n",
            "Teacher_23 training loss: 0.5429055659394515\n",
            "Teacher_24 training loss: 0.5355585233161324\n",
            "Teacher_25 training loss: 0.5854164565864363\n",
            "Teacher_26 training loss: 0.6031273368157839\n",
            "Teacher_27 training loss: 0.6465808858996943\n",
            "Teacher_28 training loss: 0.5802611551786724\n",
            "Teacher_29 training loss: 0.5417433349709762\n",
            "Teacher_30 training loss: 0.5863670894974157\n",
            "Teacher_31 training loss: 0.5226116823522668\n",
            "Teacher_32 training loss: 0.5722813496464177\n",
            "Teacher_33 training loss: 0.5840755089333183\n",
            "Teacher_34 training loss: 0.6092887646273563\n",
            "Teacher_35 training loss: 0.5623939758852908\n",
            "Teacher_36 training loss: 0.5547189947805906\n",
            "Teacher_37 training loss: 0.5507780815425672\n",
            "Teacher_38 training loss: 0.6112025176223955\n",
            "Teacher_39 training loss: 0.6061191731377652\n",
            "Teacher_40 training loss: 0.5875221569287149\n",
            "Teacher_41 training loss: 0.5081565678119659\n",
            "Teacher_42 training loss: 0.6662802994251251\n",
            "Teacher_43 training loss: 0.5785796642303467\n",
            "Teacher_44 training loss: 0.5410021935638628\n",
            "Teacher_45 training loss: 0.5796839532099272\n",
            "Teacher_46 training loss: 0.5939260187901949\n",
            "Teacher_47 training loss: 0.5186372116992348\n",
            "Teacher_48 training loss: 0.4982530066841527\n",
            "Teacher_49 training loss: 0.4549764677097923\n",
            "Teacher_50 training loss: 0.4400885873719266\n"
          ]
        }
      ],
      "source": [
        "num_teachers = 50\n",
        "num_examples = len(trainset)//num_teachers\n",
        "epochs = 10\n",
        "\n",
        "get_teacher_preds(num_teachers, num_examples, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E9_bZlbDefC",
        "outputId": "59f7b354-0632-482e-e048-de0360a4df6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/teachers 50\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNstgEpPGZCK",
        "outputId": "a79eae19-3189-4970-9723-a951aa03b3c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 6000)\n"
          ]
        }
      ],
      "source": [
        "num_teachers = 50\n",
        "\n",
        "preds = []\n",
        "for i in range(num_teachers): \n",
        "    file_name = 'pred_' + str(i+1) + '.pt'\n",
        "    pred = torch.load(file_name).cpu().numpy()\n",
        "    preds.append(pred)    \n",
        "    teacher_preds = np.vstack((preds))\n",
        "print(teacher_preds.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_qMGJ9IXGZCq"
      },
      "outputs": [],
      "source": [
        "def aggragate_teacher_preds(epsilon):  \n",
        "    \n",
        "    labels = np.array([]).astype(int)\n",
        "    for pred in np.transpose(teacher_preds):   \n",
        "        label_counts = np.bincount(pred, minlength=10)    \n",
        "        beta = 1 / epsilon\n",
        "\n",
        "        for i in range(len(label_counts)):\n",
        "            label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "        \n",
        "        new_label = np.argmax(label_counts)   \n",
        "        labels = np.append(labels, new_label) \n",
        "        labels =  torch.from_numpy(labels)      \n",
        "   \n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w_rhuXEzGZC4"
      },
      "outputs": [],
      "source": [
        "num_teachers, num_examples, num_labels = (50, 6000, 10)\n",
        "diff_priv_labels = aggragate_teacher_preds(0.01)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86NISKafGZDA",
        "outputId": "94878718-1c02-4e07-ffe0-ae8a557f8274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15    Training Loss: 2.30668   \n",
            "Epoch: 2/15    Training Loss: 2.30330   \n",
            "Epoch: 3/15    Training Loss: 2.30193   \n",
            "Epoch: 4/15    Training Loss: 2.30184   \n",
            "Epoch: 5/15    Training Loss: 2.30057   \n",
            "Epoch: 6/15    Training Loss: 2.29932   \n",
            "Epoch: 7/15    Training Loss: 2.29850   \n",
            "Epoch: 8/15    Training Loss: 2.29972   \n",
            "Epoch: 9/15    Training Loss: 2.29653   \n",
            "Epoch: 10/15    Training Loss: 2.29435   \n",
            "Epoch: 11/15    Training Loss: 2.29521   \n",
            "Epoch: 12/15    Training Loss: 2.29417   \n",
            "Epoch: 13/15    Training Loss: 2.29353   \n",
            "Epoch: 14/15    Training Loss: 2.29396   \n",
            "Epoch: 15/15    Training Loss: 2.29125   \n"
          ]
        }
      ],
      "source": [
        "testset.data[:6000] = torch.FloatTensor(testset.data.clone().detach().numpy()[:6000])\n",
        "testset.targets[:6000] = diff_priv_labels\n",
        "\n",
        "student_train = Subset(testset, list(range(6000)))\n",
        "student_trainloader = torch.utils.data.DataLoader(student_train, batch_size=64, num_workers=0)\n",
        "\n",
        "model_2 = Net()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
        "model_2.to(device)\n",
        "epochs = 15\n",
        "\n",
        "for e in range(epochs): \n",
        "    train_loss = 0.0    \n",
        "    for images, labels in student_trainloader:          \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()        \n",
        "        output = model_2(images)        \n",
        "        loss = criterion(output, labels)        \n",
        "        loss.backward()       \n",
        "        optimizer.step()        \n",
        "        train_loss += loss.item()\n",
        "            \n",
        "    print(\"Epoch: {}/{}   \".format(e+1, epochs),\n",
        "                      \"Training Loss: {:.5f}   \".format(train_loss/len(student_trainloader)))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PmNoVQEGZDN",
        "outputId": "6554a59b-9d9e-4e75-e055-25ff38fa506e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.16042    Test Accuracy: 0.47793\n"
          ]
        }
      ],
      "source": [
        "student_test = Subset(testset, list(range(6000, 10000)))  \n",
        "student_testloader = torch.utils.data.DataLoader(student_test, batch_size=64, num_workers=0)\n",
        "\n",
        "test_loss = 0\n",
        "accuracy = 0\n",
        "model_2.to(device)\n",
        "model_2.eval()\n",
        "\n",
        "with torch.no_grad(): \n",
        "    for images, labels in student_testloader:              \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model_2(images)\n",
        "        test_loss += criterion(output, labels).item()        \n",
        "               \n",
        "        ps = torch.exp(output)\n",
        "        top_p, top_class = ps.topk(1, dim=1)        \n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "print(\"Test Loss: {:.5f}   \".format(test_loss/len(student_testloader)),\n",
        "      \"Test Accuracy: {:.5f}\".format(accuracy/len(student_testloader)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oRjV29PcnQ5d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}