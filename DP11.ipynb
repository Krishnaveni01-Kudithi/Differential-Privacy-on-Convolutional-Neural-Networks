{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWYlf89SGZBy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39NqjSnGZB3",
        "outputId": "e2790434-f66d-47d7-fbcd-51bd909e789f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 107658614.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 98006224.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26302565.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14476085.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Trainset size: 60000\n",
            "Testset size: 10000\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "print('Trainset size:', len(trainset))\n",
        "print('Testset size:', len(testset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXMUwQxBd19k",
        "outputId": "67bbb677-04d4-4973-df45-bf2942754d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "model = Net()\n",
        "print(model)       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9H4Jc-SaDefA"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"gpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q7GddvNMGZCC"
      },
      "outputs": [],
      "source": [
        "def get_teacher_preds(num_teachers, num_examples, epochs): \n",
        "    dirName = \"teachers \" + str(num_teachers)\n",
        "    if not os.path.exists(dirName):\n",
        "        os.mkdir(dirName)\n",
        "        \n",
        "    os.chdir(dirName) \n",
        "    \n",
        "    for i in range(num_teachers):\n",
        "        train_idx = list(range(i * num_examples, (i+1) * num_examples))\n",
        "        train = Subset(trainset, train_idx)\n",
        "        trainloader = torch.utils.data.DataLoader(train, batch_size=64, num_workers=0)      \n",
        "       \n",
        "        model = Net()\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)        \n",
        "        model.to(device)\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            running_loss = 0\n",
        "            for images, labels in trainloader:                 \n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()        \n",
        "                output = model(images)\n",
        "                loss = criterion(output, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()        \n",
        "                running_loss += loss.item() \n",
        "                \n",
        "        print(\"Teacher_\" + str(i+1) + \" training loss:\", running_loss/len(trainloader))  \n",
        "        \n",
        "        student_idx = list(range(0, 6000))      \n",
        "        student_data = Subset(testset, student_idx)  \n",
        "        student_loader = torch.utils.data.DataLoader(student_data, batch_size=64, num_workers=0)\n",
        "        \n",
        "        model.to(device) \n",
        "        model.eval()\n",
        "        \n",
        "        outputs = torch.zeros(0, dtype=torch.long).to(device)\n",
        "        for images, labels in student_loader:             \n",
        "            images, labels = images.to(device), labels.to(device)                             \n",
        "            output = model.forward(images)\n",
        "            pred = torch.argmax(torch.exp(output), dim=1)\n",
        "            outputs = torch.cat((outputs, pred))            \n",
        "    \n",
        "        file_name = 'pred_' + str(i+1) + '.pt'  \n",
        "        torch.save(outputs, file_name)          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyfFIPO1DefC",
        "outputId": "d4fb406e-4f16-4bde-a058-cac3553ff0a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher_1 training loss: 0.5617570845704329\n",
            "Teacher_2 training loss: 0.48331134570272344\n",
            "Teacher_3 training loss: 0.5009780683015522\n",
            "Teacher_4 training loss: 0.5198076085040444\n",
            "Teacher_5 training loss: 0.6370799902238344\n",
            "Teacher_6 training loss: 0.4634491879689066\n",
            "Teacher_7 training loss: 0.6397651684911627\n",
            "Teacher_8 training loss: 0.7171247648565393\n",
            "Teacher_9 training loss: 0.4881539140876971\n",
            "Teacher_10 training loss: 0.5365479933588129\n",
            "Teacher_11 training loss: 0.6144439876079559\n",
            "Teacher_12 training loss: 0.5466022271859018\n",
            "Teacher_13 training loss: 0.49998532784612554\n",
            "Teacher_14 training loss: 0.6119667668091623\n",
            "Teacher_15 training loss: 0.6080184061276285\n",
            "Teacher_16 training loss: 0.5515544979195846\n",
            "Teacher_17 training loss: 0.5638958749018217\n",
            "Teacher_18 training loss: 0.5518312140514976\n",
            "Teacher_19 training loss: 0.5143919806731375\n",
            "Teacher_20 training loss: 0.5590498949352064\n",
            "Teacher_21 training loss: 0.5766248012843885\n",
            "Teacher_22 training loss: 0.47319592457068593\n",
            "Teacher_23 training loss: 0.5816280653602198\n",
            "Teacher_24 training loss: 0.49925406983024195\n",
            "Teacher_25 training loss: 0.6734218346445184\n",
            "Teacher_26 training loss: 0.5371383961878324\n",
            "Teacher_27 training loss: 0.5718193634560234\n",
            "Teacher_28 training loss: 0.4772042239967145\n",
            "Teacher_29 training loss: 0.5702776030490273\n",
            "Teacher_30 training loss: 0.5781046857959345\n",
            "Teacher_31 training loss: 0.5445745759888699\n",
            "Teacher_32 training loss: 0.5659894049167633\n",
            "Teacher_33 training loss: 0.5210911584527869\n",
            "Teacher_34 training loss: 0.594359815120697\n",
            "Teacher_35 training loss: 0.5560743416610517\n",
            "Teacher_36 training loss: 0.5709068383041181\n",
            "Teacher_37 training loss: 0.5832372919509285\n",
            "Teacher_38 training loss: 0.5361352907983881\n",
            "Teacher_39 training loss: 0.6023220513996325\n",
            "Teacher_40 training loss: 0.5835590111581903\n",
            "Teacher_41 training loss: 0.5060712601009169\n",
            "Teacher_42 training loss: 0.5861832954381642\n",
            "Teacher_43 training loss: 0.6133376516793904\n",
            "Teacher_44 training loss: 0.5801040655688235\n",
            "Teacher_45 training loss: 0.5310725491297873\n",
            "Teacher_46 training loss: 0.5318522594477001\n",
            "Teacher_47 training loss: 0.48837833028090627\n",
            "Teacher_48 training loss: 0.5413129878671545\n",
            "Teacher_49 training loss: 0.4911353815543024\n",
            "Teacher_50 training loss: 0.40097977926856593\n"
          ]
        }
      ],
      "source": [
        "num_teachers = 50\n",
        "num_examples = len(trainset)//num_teachers\n",
        "epochs = 10\n",
        "\n",
        "get_teacher_preds(num_teachers, num_examples, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E9_bZlbDefC",
        "outputId": "809c54f3-48c1-40b7-d9f4-08a7f6c1a957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/teachers 50\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNstgEpPGZCK",
        "outputId": "1ab45f99-ba4d-4dfc-8d1d-c2f4fb5b177a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 6000)\n"
          ]
        }
      ],
      "source": [
        "num_teachers = 50\n",
        "\n",
        "preds = []\n",
        "for i in range(num_teachers): \n",
        "    file_name = 'pred_' + str(i+1) + '.pt'\n",
        "    pred = torch.load(file_name).cpu().numpy()\n",
        "    preds.append(pred)    \n",
        "    teacher_preds = np.vstack((preds))\n",
        "print(teacher_preds.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_qMGJ9IXGZCq"
      },
      "outputs": [],
      "source": [
        "def aggragate_teacher_preds(epsilon):  \n",
        "    \n",
        "    labels = np.array([]).astype(int)\n",
        "    for pred in np.transpose(teacher_preds):   \n",
        "        label_counts = np.bincount(pred, minlength=10)    \n",
        "        beta = 1 / epsilon\n",
        "\n",
        "        for i in range(len(label_counts)):\n",
        "            label_counts[i] += np.random.exponential(1/beta)\n",
        "        \n",
        "        new_label = np.argmax(label_counts)   \n",
        "        labels = np.append(labels, new_label) \n",
        "        labels =  torch.from_numpy(labels)      \n",
        "   \n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w_rhuXEzGZC4"
      },
      "outputs": [],
      "source": [
        "num_teachers, num_examples, num_labels = (50, 6000, 10)\n",
        "diff_priv_labels = aggragate_teacher_preds(8.03)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86NISKafGZDA",
        "outputId": "a79ba6f3-4eb9-425c-ce3b-224c26d8c3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15    Training Loss: 1.71231   \n",
            "Epoch: 2/15    Training Loss: 0.78349   \n",
            "Epoch: 3/15    Training Loss: 0.60913   \n",
            "Epoch: 4/15    Training Loss: 0.53018   \n",
            "Epoch: 5/15    Training Loss: 0.50166   \n",
            "Epoch: 6/15    Training Loss: 0.48367   \n",
            "Epoch: 7/15    Training Loss: 0.47055   \n",
            "Epoch: 8/15    Training Loss: 0.45266   \n",
            "Epoch: 9/15    Training Loss: 0.44410   \n",
            "Epoch: 10/15    Training Loss: 0.41617   \n",
            "Epoch: 11/15    Training Loss: 0.42417   \n",
            "Epoch: 12/15    Training Loss: 0.41986   \n",
            "Epoch: 13/15    Training Loss: 0.42107   \n",
            "Epoch: 14/15    Training Loss: 0.39421   \n",
            "Epoch: 15/15    Training Loss: 0.40134   \n"
          ]
        }
      ],
      "source": [
        "testset.data[:6000] = torch.FloatTensor(testset.data.clone().detach().numpy()[:6000])\n",
        "testset.targets[:6000] = diff_priv_labels\n",
        "\n",
        "student_train = Subset(testset, list(range(6000)))\n",
        "student_trainloader = torch.utils.data.DataLoader(student_train, batch_size=64, num_workers=0)\n",
        "\n",
        "model_2 = Net()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
        "model_2.to(device)\n",
        "epochs = 15\n",
        "\n",
        "for e in range(epochs): \n",
        "    train_loss = 0.0    \n",
        "    for images, labels in student_trainloader:          \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()        \n",
        "        output = model_2(images)        \n",
        "        loss = criterion(output, labels)        \n",
        "        loss.backward()       \n",
        "        optimizer.step()        \n",
        "        train_loss += loss.item()\n",
        "            \n",
        "    print(\"Epoch: {}/{}   \".format(e+1, epochs),\n",
        "                      \"Training Loss: {:.5f}   \".format(train_loss/len(student_trainloader)))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PmNoVQEGZDN",
        "outputId": "7e480730-6f47-47db-cf28-8c02773b2244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.15091    Test Accuracy: 0.95362\n"
          ]
        }
      ],
      "source": [
        "student_test = Subset(testset, list(range(6000, 10000)))  \n",
        "student_testloader = torch.utils.data.DataLoader(student_test, batch_size=64, num_workers=0)\n",
        "\n",
        "test_loss = 0\n",
        "accuracy = 0\n",
        "model_2.to(device)\n",
        "model_2.eval()\n",
        "\n",
        "with torch.no_grad(): \n",
        "    for images, labels in student_testloader:              \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model_2(images)\n",
        "        test_loss += criterion(output, labels).item()        \n",
        "               \n",
        "        ps = torch.exp(output)\n",
        "        top_p, top_class = ps.topk(1, dim=1)        \n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "print(\"Test Loss: {:.5f}   \".format(test_loss/len(student_testloader)),\n",
        "      \"Test Accuracy: {:.5f}\".format(accuracy/len(student_testloader)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}