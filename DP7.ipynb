{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWYlf89SGZBy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39NqjSnGZB3",
        "outputId": "540be554-4e7c-42df-ee5d-855402dafaa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 136409965.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 102657367.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 37655127.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4142319.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Trainset size: 60000\n",
            "Testset size: 10000\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "print('Trainset size:', len(trainset))\n",
        "print('Testset size:', len(testset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXMUwQxBd19k",
        "outputId": "4665a38d-b97a-4260-9ef1-54606f681279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "model = Net()\n",
        "print(model)       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9H4Jc-SaDefA"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"gpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q7GddvNMGZCC"
      },
      "outputs": [],
      "source": [
        "def get_teacher_preds(num_teachers, num_examples, epochs): \n",
        "    dirName = \"teachers \" + str(num_teachers)\n",
        "    if not os.path.exists(dirName):\n",
        "        os.mkdir(dirName)\n",
        "        \n",
        "    os.chdir(dirName) \n",
        "    \n",
        "    for i in range(num_teachers):\n",
        "        train_idx = list(range(i * num_examples, (i+1) * num_examples))\n",
        "        train = Subset(trainset, train_idx)\n",
        "        trainloader = torch.utils.data.DataLoader(train, batch_size=64, num_workers=0)      \n",
        "       \n",
        "        model = Net()\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)        \n",
        "        model.to(device)\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            running_loss = 0\n",
        "            for images, labels in trainloader:                 \n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()        \n",
        "                output = model(images)\n",
        "                loss = criterion(output, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()        \n",
        "                running_loss += loss.item() \n",
        "                \n",
        "        print(\"Teacher_\" + str(i+1) + \" training loss:\", running_loss/len(trainloader))  \n",
        "        \n",
        "        student_idx = list(range(0, 6000))      \n",
        "        student_data = Subset(testset, student_idx)  \n",
        "        student_loader = torch.utils.data.DataLoader(student_data, batch_size=64, num_workers=0)\n",
        "        \n",
        "        model.to(device) \n",
        "        model.eval()\n",
        "        \n",
        "        outputs = torch.zeros(0, dtype=torch.long).to(device)\n",
        "        for images, labels in student_loader:             \n",
        "            images, labels = images.to(device), labels.to(device)                             \n",
        "            output = model.forward(images)\n",
        "            pred = torch.argmax(torch.exp(output), dim=1)\n",
        "            outputs = torch.cat((outputs, pred))            \n",
        "    \n",
        "        file_name = 'pred_' + str(i+1) + '.pt'  \n",
        "        torch.save(outputs, file_name)          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyfFIPO1DefC",
        "outputId": "fae132b6-a562-4860-9c1d-fdb81d6de786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher_1 training loss: 0.5253555617834392\n",
            "Teacher_2 training loss: 0.505951275950984\n",
            "Teacher_3 training loss: 0.5200359397812894\n",
            "Teacher_4 training loss: 0.4836801353253816\n",
            "Teacher_5 training loss: 0.5585805428655524\n",
            "Teacher_6 training loss: 0.49722732368268463\n",
            "Teacher_7 training loss: 0.5320971914027867\n",
            "Teacher_8 training loss: 0.6196737712935397\n",
            "Teacher_9 training loss: 0.5051776437382949\n",
            "Teacher_10 training loss: 0.5492752853192782\n",
            "Teacher_11 training loss: 0.6141578586477983\n",
            "Teacher_12 training loss: 0.526649828019895\n",
            "Teacher_13 training loss: 0.5586872226313541\n",
            "Teacher_14 training loss: 0.5142468151293302\n",
            "Teacher_15 training loss: 0.5495706733904386\n",
            "Teacher_16 training loss: 0.5269742592384941\n",
            "Teacher_17 training loss: 0.587367511109302\n",
            "Teacher_18 training loss: 0.5163918310090115\n",
            "Teacher_19 training loss: 0.572847560832375\n",
            "Teacher_20 training loss: 0.5464822681326615\n",
            "Teacher_21 training loss: 0.539133464035235\n",
            "Teacher_22 training loss: 0.4593458301142642\n",
            "Teacher_23 training loss: 0.5704531669616699\n",
            "Teacher_24 training loss: 0.5466603762225101\n",
            "Teacher_25 training loss: 0.6130607065401579\n",
            "Teacher_26 training loss: 0.6172497931279635\n",
            "Teacher_27 training loss: 0.6163687533453891\n",
            "Teacher_28 training loss: 0.5290849036292026\n",
            "Teacher_29 training loss: 0.4974212834709569\n",
            "Teacher_30 training loss: 0.5206138428888822\n",
            "Teacher_31 training loss: 0.523529411930787\n",
            "Teacher_32 training loss: 0.5601530545636227\n",
            "Teacher_33 training loss: 0.6016723999851629\n",
            "Teacher_34 training loss: 0.5435655760137659\n",
            "Teacher_35 training loss: 0.5954906234615728\n",
            "Teacher_36 training loss: 0.5694103570360887\n",
            "Teacher_37 training loss: 0.517759349785353\n",
            "Teacher_38 training loss: 0.5383853128081874\n",
            "Teacher_39 training loss: 0.5439421440425672\n",
            "Teacher_40 training loss: 0.5775039258756136\n",
            "Teacher_41 training loss: 0.453726138723524\n",
            "Teacher_42 training loss: 0.6066910718616686\n",
            "Teacher_43 training loss: 0.5117823638414082\n",
            "Teacher_44 training loss: 0.5282291032766041\n",
            "Teacher_45 training loss: 0.5753832616304096\n",
            "Teacher_46 training loss: 0.5401815521089655\n",
            "Teacher_47 training loss: 0.5035373333253359\n",
            "Teacher_48 training loss: 0.5185459162059584\n",
            "Teacher_49 training loss: 0.45814352286489385\n",
            "Teacher_50 training loss: 0.41650862678101186\n"
          ]
        }
      ],
      "source": [
        "num_teachers = 50\n",
        "num_examples = len(trainset)//num_teachers\n",
        "epochs = 10\n",
        "\n",
        "get_teacher_preds(num_teachers, num_examples, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E9_bZlbDefC",
        "outputId": "687bc38f-0fc4-467c-9393-2219c68b3f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/teachers 50\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNstgEpPGZCK",
        "outputId": "f3f95cc9-443c-4333-9cfc-8617b091d830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 6000)\n"
          ]
        }
      ],
      "source": [
        "num_teachers = 50\n",
        "\n",
        "preds = []\n",
        "for i in range(num_teachers): \n",
        "    file_name = 'pred_' + str(i+1) + '.pt'\n",
        "    pred = torch.load(file_name).cpu().numpy()\n",
        "    preds.append(pred)    \n",
        "    teacher_preds = np.vstack((preds))\n",
        "print(teacher_preds.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_qMGJ9IXGZCq"
      },
      "outputs": [],
      "source": [
        "def aggragate_teacher_preds(epsilon):  \n",
        "    \n",
        "    labels = np.array([]).astype(int)\n",
        "    for pred in np.transpose(teacher_preds):   \n",
        "        label_counts = np.bincount(pred, minlength=10)    \n",
        "        beta = 1 / epsilon\n",
        "\n",
        "        for i in range(len(label_counts)):\n",
        "            label_counts[i] += np.random.normal(0, beta, 1)\n",
        "        \n",
        "        new_label = np.argmax(label_counts)   \n",
        "        labels = np.append(labels, new_label) \n",
        "        labels =  torch.from_numpy(labels)      \n",
        "   \n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w_rhuXEzGZC4"
      },
      "outputs": [],
      "source": [
        "num_teachers, num_examples, num_labels = (50, 6000, 10)\n",
        "diff_priv_labels = aggragate_teacher_preds(2.04)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86NISKafGZDA",
        "outputId": "6d19e7fe-9017-42ca-e277-1dff1bfaa4db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15    Training Loss: 1.60835   \n",
            "Epoch: 2/15    Training Loss: 0.62167   \n",
            "Epoch: 3/15    Training Loss: 0.44638   \n",
            "Epoch: 4/15    Training Loss: 0.38034   \n",
            "Epoch: 5/15    Training Loss: 0.35886   \n",
            "Epoch: 6/15    Training Loss: 0.32071   \n",
            "Epoch: 7/15    Training Loss: 0.29952   \n",
            "Epoch: 8/15    Training Loss: 0.27389   \n",
            "Epoch: 9/15    Training Loss: 0.28523   \n",
            "Epoch: 10/15    Training Loss: 0.27455   \n",
            "Epoch: 11/15    Training Loss: 0.25924   \n",
            "Epoch: 12/15    Training Loss: 0.24574   \n",
            "Epoch: 13/15    Training Loss: 0.25001   \n",
            "Epoch: 14/15    Training Loss: 0.23663   \n",
            "Epoch: 15/15    Training Loss: 0.23603   \n"
          ]
        }
      ],
      "source": [
        "testset.data[:6000] = torch.FloatTensor(testset.data.clone().detach().numpy()[:6000])\n",
        "testset.targets[:6000] = diff_priv_labels\n",
        "\n",
        "student_train = Subset(testset, list(range(6000)))\n",
        "student_trainloader = torch.utils.data.DataLoader(student_train, batch_size=64, num_workers=0)\n",
        "\n",
        "model_2 = Net()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
        "model_2.to(device)\n",
        "epochs = 15\n",
        "\n",
        "for e in range(epochs): \n",
        "    train_loss = 0.0    \n",
        "    for images, labels in student_trainloader:          \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()        \n",
        "        output = model_2(images)        \n",
        "        loss = criterion(output, labels)        \n",
        "        loss.backward()       \n",
        "        optimizer.step()        \n",
        "        train_loss += loss.item()\n",
        "            \n",
        "    print(\"Epoch: {}/{}   \".format(e+1, epochs),\n",
        "                      \"Training Loss: {:.5f}   \".format(train_loss/len(student_trainloader)))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PmNoVQEGZDN",
        "outputId": "7c0dc8d5-679b-45b7-a2a9-9a7afeb13ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.17101    Test Accuracy: 0.95139\n"
          ]
        }
      ],
      "source": [
        "student_test = Subset(testset, list(range(6000, 10000)))  \n",
        "student_testloader = torch.utils.data.DataLoader(student_test, batch_size=64, num_workers=0)\n",
        "\n",
        "test_loss = 0\n",
        "accuracy = 0\n",
        "model_2.to(device)\n",
        "model_2.eval()\n",
        "\n",
        "with torch.no_grad(): \n",
        "    for images, labels in student_testloader:              \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model_2(images)\n",
        "        test_loss += criterion(output, labels).item()        \n",
        "               \n",
        "        ps = torch.exp(output)\n",
        "        top_p, top_class = ps.topk(1, dim=1)        \n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "print(\"Test Loss: {:.5f}   \".format(test_loss/len(student_testloader)),\n",
        "      \"Test Accuracy: {:.5f}\".format(accuracy/len(student_testloader)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}