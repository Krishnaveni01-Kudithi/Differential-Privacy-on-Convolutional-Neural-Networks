{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWYlf89SGZBy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39NqjSnGZB3",
        "outputId": "c07f50d2-c623-4bf2-ad50-12f492b75199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 47173347.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 75899557.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 20713393.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4726005.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainset size: 60000\n",
            "Testset size: 10000\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "print('Trainset size:', len(trainset))\n",
        "print('Testset size:', len(testset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXMUwQxBd19k",
        "outputId": "4449674e-4eb4-4ee7-950e-91ac18fd6bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "model = Net()\n",
        "print(model)       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9H4Jc-SaDefA"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q7GddvNMGZCC"
      },
      "outputs": [],
      "source": [
        "def get_teacher_preds(num_teachers, num_examples, epochs): \n",
        "    dirName = \"teachers \" + str(num_teachers)\n",
        "    if not os.path.exists(dirName):\n",
        "        os.mkdir(dirName)\n",
        "        \n",
        "    os.chdir(dirName) \n",
        "    \n",
        "    for i in range(num_teachers):\n",
        "        train_idx = list(range(i * num_examples, (i+1) * num_examples))\n",
        "        train = Subset(trainset, train_idx)\n",
        "        trainloader = torch.utils.data.DataLoader(train, batch_size=64, num_workers=0)      \n",
        "       \n",
        "        model = Net()\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)        \n",
        "        model.to(device)\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            running_loss = 0\n",
        "            for images, labels in trainloader:                 \n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()        \n",
        "                output = model(images)\n",
        "                loss = criterion(output, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()        \n",
        "                running_loss += loss.item() \n",
        "                \n",
        "        print(\"Teacher_\" + str(i+1) + \" training loss:\", running_loss/len(trainloader))  \n",
        "        \n",
        "        student_idx = list(range(0, 6000))      \n",
        "        student_data = Subset(testset, student_idx)  \n",
        "        student_loader = torch.utils.data.DataLoader(student_data, batch_size=64, num_workers=0)\n",
        "        \n",
        "        model.to(device) \n",
        "        model.eval()\n",
        "        \n",
        "        outputs = torch.zeros(0, dtype=torch.long).to(device)\n",
        "        for images, labels in student_loader:             \n",
        "            images, labels = images.to(device), labels.to(device)                             \n",
        "            output = model.forward(images)\n",
        "            pred = torch.argmax(torch.exp(output), dim=1)\n",
        "            outputs = torch.cat((outputs, pred))            \n",
        "    \n",
        "        file_name = 'pred_' + str(i+1) + '.pt'  \n",
        "        torch.save(outputs, file_name)          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyfFIPO1DefC",
        "outputId": "3e562b9f-583f-4554-d147-5cb741554a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher_1 training loss: 0.47146135411764445\n",
            "Teacher_2 training loss: 0.4953027257793828\n",
            "Teacher_3 training loss: 0.531466250356875\n",
            "Teacher_4 training loss: 0.5089226223920521\n",
            "Teacher_5 training loss: 0.534604737633153\n",
            "Teacher_6 training loss: 0.5183183488092924\n",
            "Teacher_7 training loss: 0.7267155992357355\n",
            "Teacher_8 training loss: 0.5955008550694114\n",
            "Teacher_9 training loss: 0.510783231572101\n",
            "Teacher_10 training loss: 0.48521350088872406\n",
            "Teacher_11 training loss: 0.5428287857457211\n",
            "Teacher_12 training loss: 0.5570392200821325\n",
            "Teacher_13 training loss: 0.5597347005417472\n",
            "Teacher_14 training loss: 0.5766650579477611\n",
            "Teacher_15 training loss: 0.5969997675795304\n",
            "Teacher_16 training loss: 0.48472936373007924\n",
            "Teacher_17 training loss: 0.5232906561148795\n",
            "Teacher_18 training loss: 0.5619826771711048\n",
            "Teacher_19 training loss: 0.5742829520451395\n",
            "Teacher_20 training loss: 0.5331527371155588\n",
            "Teacher_21 training loss: 0.5782566227410969\n",
            "Teacher_22 training loss: 0.5557979737457476\n",
            "Teacher_23 training loss: 0.4981159089427245\n",
            "Teacher_24 training loss: 0.5126784616395047\n",
            "Teacher_25 training loss: 0.6157639120754442\n",
            "Teacher_26 training loss: 0.6566576095003831\n",
            "Teacher_27 training loss: 0.5781782363590441\n",
            "Teacher_28 training loss: 0.5732348922051882\n",
            "Teacher_29 training loss: 0.492166131734848\n",
            "Teacher_30 training loss: 0.5675698801090843\n",
            "Teacher_31 training loss: 0.4667663385993556\n",
            "Teacher_32 training loss: 0.5665805402554964\n",
            "Teacher_33 training loss: 0.6337525954372004\n",
            "Teacher_34 training loss: 0.5951199531555176\n",
            "Teacher_35 training loss: 0.532609359998452\n",
            "Teacher_36 training loss: 0.574225315922185\n",
            "Teacher_37 training loss: 0.5151147230675346\n",
            "Teacher_38 training loss: 0.557523532917625\n",
            "Teacher_39 training loss: 0.572149606127488\n",
            "Teacher_40 training loss: 0.5066377567617517\n",
            "Teacher_41 training loss: 0.48893899823489945\n",
            "Teacher_42 training loss: 0.6572671548316353\n",
            "Teacher_43 training loss: 0.5360391202725863\n",
            "Teacher_44 training loss: 0.5371429277093787\n",
            "Teacher_45 training loss: 0.5504318271812639\n",
            "Teacher_46 training loss: 0.5506742345659357\n",
            "Teacher_47 training loss: 0.42718223051020976\n",
            "Teacher_48 training loss: 0.5628545958744852\n",
            "Teacher_49 training loss: 0.4274659854801078\n",
            "Teacher_50 training loss: 0.4253205994242116\n"
          ]
        }
      ],
      "source": [
        "num_teachers = 50\n",
        "num_examples = len(trainset)//num_teachers\n",
        "epochs = 10\n",
        "\n",
        "get_teacher_preds(num_teachers, num_examples, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E9_bZlbDefC",
        "outputId": "b93d5a1f-6f8c-4ff9-da6e-b98ea4adb8e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/teachers 50\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNstgEpPGZCK",
        "outputId": "c2d5808b-8d94-46c4-b0cd-7dd4e057d77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 6000)\n"
          ]
        }
      ],
      "source": [
        "num_teachers = 50\n",
        "\n",
        "preds = []\n",
        "for i in range(num_teachers): \n",
        "    file_name = 'pred_' + str(i+1) + '.pt'\n",
        "    pred = torch.load(file_name).cpu().numpy()\n",
        "    preds.append(pred)    \n",
        "    teacher_preds = np.vstack((preds))\n",
        "print(teacher_preds.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_qMGJ9IXGZCq"
      },
      "outputs": [],
      "source": [
        "def aggragate_teacher_preds(epsilon):  \n",
        "    \n",
        "    labels = np.array([]).astype(int)\n",
        "    for pred in np.transpose(teacher_preds):   \n",
        "        label_counts = np.bincount(pred, minlength=10)    \n",
        "        beta = 1 / epsilon\n",
        "\n",
        "        for i in range(len(label_counts)):\n",
        "            label_counts[i] += np.random.normal(0, beta, 1)\n",
        "        \n",
        "        new_label = np.argmax(label_counts)   \n",
        "        labels = np.append(labels, new_label) \n",
        "        labels =  torch.from_numpy(labels)      \n",
        "   \n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w_rhuXEzGZC4"
      },
      "outputs": [],
      "source": [
        "num_teachers, num_examples, num_labels = (50, 6000, 10)\n",
        "diff_priv_labels = aggragate_teacher_preds(0.01)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86NISKafGZDA",
        "outputId": "7b87e68a-1ebe-42df-dc7b-8bbdf0e2bce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15    Training Loss: 2.30559   \n",
            "Epoch: 2/15    Training Loss: 2.30049   \n",
            "Epoch: 3/15    Training Loss: 2.29757   \n",
            "Epoch: 4/15    Training Loss: 2.29438   \n",
            "Epoch: 5/15    Training Loss: 2.28812   \n",
            "Epoch: 6/15    Training Loss: 2.28323   \n",
            "Epoch: 7/15    Training Loss: 2.28285   \n",
            "Epoch: 8/15    Training Loss: 2.27882   \n",
            "Epoch: 9/15    Training Loss: 2.27874   \n",
            "Epoch: 10/15    Training Loss: 2.27374   \n",
            "Epoch: 11/15    Training Loss: 2.27409   \n",
            "Epoch: 12/15    Training Loss: 2.27047   \n",
            "Epoch: 13/15    Training Loss: 2.26833   \n",
            "Epoch: 14/15    Training Loss: 2.26693   \n",
            "Epoch: 15/15    Training Loss: 2.26522   \n"
          ]
        }
      ],
      "source": [
        "testset.data[:6000] = torch.FloatTensor(testset.data.clone().detach().numpy()[:6000])\n",
        "testset.targets[:6000] = diff_priv_labels\n",
        "\n",
        "student_train = Subset(testset, list(range(6000)))\n",
        "student_trainloader = torch.utils.data.DataLoader(student_train, batch_size=64, num_workers=0)\n",
        "\n",
        "model_2 = Net()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
        "model_2.to(device)\n",
        "epochs = 15\n",
        "\n",
        "for e in range(epochs): \n",
        "    train_loss = 0.0    \n",
        "    for images, labels in student_trainloader:          \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()        \n",
        "        output = model_2(images)        \n",
        "        loss = criterion(output, labels)        \n",
        "        loss.backward()       \n",
        "        optimizer.step()        \n",
        "        train_loss += loss.item()\n",
        "            \n",
        "    print(\"Epoch: {}/{}   \".format(e+1, epochs),\n",
        "                      \"Training Loss: {:.5f}   \".format(train_loss/len(student_trainloader)))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PmNoVQEGZDN",
        "outputId": "c7bf36b8-d986-46ce-9e00-51579e01856c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-34973861982a>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.01536    Test Accuracy: 0.69221\n"
          ]
        }
      ],
      "source": [
        "student_test = Subset(testset, list(range(6000, 10000)))  \n",
        "student_testloader = torch.utils.data.DataLoader(student_test, batch_size=64, num_workers=0)\n",
        "\n",
        "test_loss = 0\n",
        "accuracy = 0\n",
        "model_2.to(device)\n",
        "model_2.eval()\n",
        "\n",
        "with torch.no_grad(): \n",
        "    for images, labels in student_testloader:              \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model_2(images)\n",
        "        test_loss += criterion(output, labels).item()        \n",
        "               \n",
        "        ps = torch.exp(output)\n",
        "        top_p, top_class = ps.topk(1, dim=1)        \n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "print(\"Test Loss: {:.5f}   \".format(test_loss/len(student_testloader)),\n",
        "      \"Test Accuracy: {:.5f}\".format(accuracy/len(student_testloader)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2IWDTOCmqSUu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}